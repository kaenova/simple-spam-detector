{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Pipeline  \n",
    "This pipeline has an input of string and an output of a classification number,\n",
    "either 0 or 1.  \n",
    "This pipeline includes:  \n",
    "1. String Preprocessing\n",
    "2. String Tokenizing\n",
    "3. Classifying\n",
    "4. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all models and its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported from iFest 2021 Data Cleaning Module by Yaudahlah Teams,\n",
    "# Refactored by Kaenova Mahendra Auditama (Yaudahlah Teams)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "class DataCleaning:\n",
    "  def __init__(self, stopword:list = [], slang_word:dict = {}) -> None:\n",
    "    factory     = StemmerFactory()\n",
    "    self.stemmer     = factory.create_stemmer()\n",
    "    self.stopword = stopword\n",
    "    self.slang_word = slang_word\n",
    "\n",
    "  def AddKamusAlay(self, new_dict:dict = {}):\n",
    "    if (type(new_dict) != dict): raise TypeError(\"Not a valid type\")\n",
    "    self.slang_word = self.slang_word | new_dict\n",
    "  \n",
    "  def AddStopWord(self, stopword:list = []):\n",
    "    if (type(stopword) != list): raise TypeError(\"Not a valid type\")\n",
    "    self.custom_word = self.custom_word + stopword\n",
    "    \n",
    "  def CleanDataFrame(self, df:pd.DataFrame, text_cols:str, label_cols:str, \n",
    "                     word_min:int=0, label_mapping:dict=None, dropna:bool=False):\n",
    "    \"\"\"\n",
    "    Using multiprocessing (*if available) to process data from pandas Dataframe.\n",
    "    Will be outputing a new dataframe with a processed data.\n",
    "    \"\"\"\n",
    "    print(\"Processing...\")\n",
    "    final_list_clean = []\n",
    "    final_list_dirty = []\n",
    "    final_label = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "      sentence = row[text_cols]\n",
    "      label = row[label_cols]\n",
    "      \n",
    "      # Process label\n",
    "      if label_mapping is not None:\n",
    "        if label not in label_mapping:\n",
    "          print(f\"Label {label} is not matched any label_mapping you've defined. This label will be ignored\")\n",
    "          continue      \n",
    "        clean_label = label_mapping[label]\n",
    "      else:\n",
    "        clean_label = label  \n",
    "      \n",
    "      # Process Text\n",
    "      clean_sentence = self.__cleanText__(sentence, self.slang_word,\n",
    "                                          self.stopword, self.stemmer)\n",
    "      if (clean_sentence is None):\n",
    "        print(f\"Sentence '{sentence}' is empty after processing. This sentence will be ignored\")\n",
    "        continue\n",
    "      if (len(clean_sentence.split()) < word_min):\n",
    "        continue\n",
    "      \n",
    "      final_list_clean.append(clean_sentence)\n",
    "      final_list_dirty.append(sentence)\n",
    "      final_label.append(clean_label)\n",
    "        \n",
    "    # Creating pandas dataframe\n",
    "    data = {\n",
    "      'raw': final_list_dirty,\n",
    "      'processed': final_list_clean,\n",
    "      'label': final_label\n",
    "    }\n",
    "    final_df = pd.DataFrame(data)\n",
    "    if dropna:\n",
    "      print(\"NaN Dropped\")\n",
    "      final_df = final_df.dropna(how='any')\n",
    "    final_df['processed'] = final_df['processed'].astype(str)\n",
    "    final_df['raw'] = final_df['raw'].astype(str)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "  def CleanOneText(self, text):\n",
    "    return self.__cleanText__(text, self.slang_word, self.stopword, self.stemmer)\n",
    "\n",
    "  def __cleanText__(self, text:str, slangword:dict, stopword:list, stemmer) -> str:\n",
    "    '''\n",
    "    Processing a text, deleting some web associated word, removing word from stopword list\n",
    "    and change defined slang word.\n",
    "    '''\n",
    "    # HTML and text annotation removal\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub('(@\\w+|#\\w+)','',text)\n",
    "    text = re.sub('<.*?>', '', text)  \n",
    "    temp_text = list(text)\n",
    "    for i in range(len(temp_text)):\n",
    "      if temp_text[i] in string.punctuation:\n",
    "        temp_text[i] = \" \"\n",
    "    text = ''.join(temp_text)\n",
    "    text = re.sub('[^a-zA-Z]',' ',text) \n",
    "    text = re.sub(\"\\n\",\" \",text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"(username|user|url|rt|xf|fx|xe|xa)\\s|\\s(user|url|rt|xf|fx|xe|xa)\",\"\",text)\n",
    "    text = re.sub(r'(\\w)(\\1{2,})', r\"\\1\", text)\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\",\"\",text)\n",
    "    text = re.sub('(s{2,})',' ',text)\n",
    "    text=' '.join(text.split())\n",
    "    text_split = text.split(' ')\n",
    "    final_text_split = []\n",
    "    for i in range(len(text_split)):\n",
    "      if type(text_split[i]) != str:\n",
    "        continue\n",
    "      if str(text_split[i]) in stopword:\n",
    "        continue\n",
    "      if str(text_split[i]) in slangword:\n",
    "        text_split[i] = str(slangword[text_split[i]])\n",
    "      final_text_split.append(text_split[i])\n",
    "    \n",
    "    stemmed_text = stemmer.stem(\" \".join(final_text_split))\n",
    "    \n",
    "    # just to make sure\n",
    "    if len(stemmed_text) == 0:\n",
    "      return None   \n",
    "    \n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "data_cleaner = joblib.load(\"../model/DataCleaner.pkl\")\n",
    "tokenizer = joblib.load(\"../model/CountVectorizer.pkl\")\n",
    "model = joblib.load(\"../model/GaussianNB.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictText(text:str) -> int:\n",
    "    clean_text = data_cleaner.CleanOneText(text)\n",
    "    tokenized = tokenizer.transform([clean_text]).toarray()\n",
    "    return model.predict(tokenized)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictText(\"Kaenova Mahendra Auditama dapatkan pulsa sebesar 2000 rupiah\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "591d8a998fafa6deec9d3cafe2af64d5585ee28f16b27e3e2a922df9441df6fd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
